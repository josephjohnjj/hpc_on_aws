---
- name: Validate system requirements
  hosts: ec2
  become: yes
  gather_facts: yes

  tasks:
    - name: Fail if OS is not Ubuntu 22.04
      fail:
        msg: "This playbook only supports Ubuntu 22.04"
      when: not (ansible_distribution == "Ubuntu" and ansible_distribution_version == "22.04")

    - name: Check that CPU is Intel
      shell: lscpu | grep -i "Vendor ID" | grep -i GenuineIntel
      register: intel_cpu_check
      changed_when: false
      failed_when: intel_cpu_check.rc != 0

    - name: Check that NVIDIA GPU is present
      shell: lspci | grep -i nvidia
      register: nvidia_gpu_check
      changed_when: false
      failed_when: nvidia_gpu_check.rc != 0


- name: Install Spack into /apps/spack
  hosts: ec2
  become: yes
  gather_facts: no  # already gathered
  vars:
    app_dir: /apps
    git_repo: "https://github.com/spack/spack.git"

  tasks:
    - name: Create /apps directory
      file:
        path: "{{ app_dir }}"
        state: directory
        owner: "{{ ansible_user }}"
        group: "{{ ansible_user }}"
        mode: '0755'
      when:
        - hostvars[inventory_hostname].ansible_distribution == "Ubuntu"
        - hostvars[inventory_hostname].ansible_distribution_version == "22.04"
        - hostvars[inventory_hostname].intel_cpu_check.rc == 0
        - hostvars[inventory_hostname].nvidia_gpu_check.rc == 0

    - name: Clone spack repo into /apps/spack if not present
      git:
        repo: "{{ git_repo }}"
        dest: "{{ app_dir }}/spack"
        depth: 2
        update: yes
      become_user: "{{ ansible_user }}"
      when:
        - hostvars[inventory_hostname].ansible_distribution == "Ubuntu"
        - hostvars[inventory_hostname].ansible_distribution_version == "22.04"
        - hostvars[inventory_hostname].intel_cpu_check.rc == 0
        - hostvars[inventory_hostname].nvidia_gpu_check.rc == 0


- name: Set ownership of /apps to the current user
  hosts: ec2
  become: yes
  gather_facts: yes  # needed to resolve ansible_user

  tasks:
    - name: Set recursive ownership of /apps
      file:
        path: /apps
        state: directory
        recurse: yes
        owner: "{{ ansible_user }}"
        group: "{{ ansible_user }}"
      when:
        - ansible_distribution == "Ubuntu"
        - ansible_distribution_version == "22.04"
        - hostvars[inventory_hostname].intel_cpu_check.rc == 0
        - hostvars[inventory_hostname].nvidia_gpu_check.rc == 0

- name: Make Spack environment available system-wide
  hosts: ec2
  become: yes
  gather_facts: yes

  tasks:
    - name: Create /etc/profile.d/spack.sh with Spack environment setup
      copy:
        dest: /etc/profile.d/spack.sh
        content: |
          export SPACK_ROOT=/apps/spack
          . $SPACK_ROOT/share/spack/setup-env.sh
        owner: root
        group: root
        mode: '0644'
      when:
        - ansible_distribution == "Ubuntu"
        - ansible_distribution_version == "22.04"
        - hostvars[inventory_hostname].intel_cpu_check.rc == 0
        - hostvars[inventory_hostname].nvidia_gpu_check.rc == 0

    - name: Ensure /etc/profile.d/spack.sh is readable by all
      file:
        path: /etc/profile.d/spack.sh
        mode: '0644'
      when:
        - ansible_distribution == "Ubuntu"
        - ansible_distribution_version == "22.04"
        - hostvars[inventory_hostname].intel_cpu_check.rc == 0
        - hostvars[inventory_hostname].nvidia_gpu_check.rc == 0

- name: Run a command with Spack environment activated
  shell: . /etc/profile.d/spack.sh && spack --version
  register: spack_version
  changed_when: false

- debug:
    var: spack_version.stdout



- name: Enable MIG mode on all detected NVIDIA GPUs and report status
  hosts: ec2
  become: yes
  gather_facts: no

  tasks:
    - name: Check if nvidia-smi is installed
      command: which nvidia-smi
      register: nvidia_smi_check
      changed_when: false
      failed_when: nvidia_smi_check.rc != 0
      ignore_errors: yes

    - name: Fail if nvidia-smi is not installed
      fail:
        msg: "nvidia-smi not found on this host. Cannot proceed."
      when: nvidia_smi_check.rc != 0

    - name: Get number of GPUs
      command: nvidia-smi --list-gpus
      register: gpu_list
      changed_when: false

    - name: Set GPU count fact
      set_fact:
        gpu_count: "{{ gpu_list.stdout_lines | length }}"

    - name: Debug GPU count
      debug:
        msg: "Detected {{ gpu_count }} GPUs"

    - name: Enable MIG mode on each GPU
      shell: nvidia-smi -i {{ item }} -mig 1
      loop: "{{ query('sequence', '0,' ~ (gpu_count - 1) ) }}"
      register: mig_enable_results
      ignore_errors: yes

    - name: Report results of enabling MIG
      debug:
        var: mig_enable_results.results

    - name: Show current MIG status for all GPUs
      shell: nvidia-smi -q -d MIG
      register: mig_status
      changed_when: false

    - name: Print MIG status
      debug:
        var: mig_status.stdout_lines

    - name: Reboot the machine to apply MIG mode
      reboot:
        reboot_timeout: 600
        test_command: nvidia-smi


- name: Create 3 MIG instances on each GPU and debug GPU count before/after
  hosts: ec2
  become: yes
  gather_facts: no

  tasks:
    - name: Check if nvidia-smi is installed
      command: which nvidia-smi
      register: nvidia_smi_check
      changed_when: false
      failed_when: nvidia_smi_check.rc != 0
      ignore_errors: yes

    - name: Fail if nvidia-smi is not installed
      fail:
        msg: "nvidia-smi not found on this host. Cannot proceed."
      when: nvidia_smi_check.rc != 0

    - name: Get number of GPUs BEFORE MIG creation
      command: nvidia-smi --list-gpus
      register: gpu_list_before
      changed_when: false

    - name: Print number of GPUs BEFORE MIG creation
      debug:
        msg: "Number of GPUs before MIG creation: {{ gpu_list_before.stdout_lines | length }}"

    - name: Set GPU count fact
      set_fact:
        gpu_count: "{{ gpu_list_before.stdout_lines | length }}"

    - name: Create 3 MIG instances on each GPU
      shell: |
        for _ in {1..3}; do
          nvidia-smi mig -cgi 14 -C -i {{ item }}
        done
      loop: "{{ query('sequence', '0,' ~ (gpu_count - 1) ) }}"
      register: mig_create_results
      ignore_errors: yes

    - name: Report MIG instance creation results
      debug:
        var: mig_create_results.results

    - name: Get number of GPUs AFTER MIG creation
      command: nvidia-smi --list-gpus
      register: gpu_list_after
      changed_when: false

    - name: Print number of GPUs AFTER MIG creation
      debug:
        msg: "Number of GPUs after MIG creation: {{ gpu_list_after.stdout_lines | length }}"


- name: Create CUDA user group and users
  hosts: ec2
  become: yes
  gather_facts: no
  vars:
    users_to_add:
      - user1
      - user
    default_password: "cudauser"  # change this to a secure password

  tasks:
    - name: Create supergroup 'cudauser'
      group:
        name: cudauser
        state: present
      when:
        - hostvars[inventory_hostname].ansible_distribution == "Ubuntu"
        - hostvars[inventory_hostname].ansible_distribution_version == "22.04"
        - hostvars[inventory_hostname].intel_cpu_check.rc == 0
        - hostvars[inventory_hostname].nvidia_gpu_check.rc == 0

    - name: Create users and add to cudauser group
      user:
        name: "{{ item }}"
        groups: cudauser
        append: yes
        password: "{{ default_password | password_hash('sha512') }}"
        shell: /bin/bash
        state: present
      loop: "{{ users_to_add }}"
      when:
        - hostvars[inventory_hostname].ansible_distribution == "Ubuntu"
        - hostvars[inventory_hostname].ansible_distribution_version == "22.04"
        - hostvars[inventory_hostname].intel_cpu_check.rc == 0
        - hostvars[inventory_hostname].nvidia_gpu_check.rc == 0
